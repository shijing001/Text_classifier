{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "## load data from json\n",
    "with open('original_labelled_data.json', 'r') as jf:\n",
    "    data=json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences,labels=data['sentences'], data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sentences, labels, test_size=0.2, random_state=2020, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,val=pd.DataFrame(columns=['sentences', 'labels']),pd.DataFrame(columns=['sentences', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sentences'], train['labels']=X_train,y_train\n",
    "val['sentences'], val['labels']=X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# set up fields\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = string.lower()    ## lower-wise case\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()\n",
    "\n",
    "def process_document(sentence):\n",
    "    \"\"\"\n",
    "    process a document \n",
    "    :params[in]: sentence, a string or list of strings\n",
    "    :params[out]: tokens, list of tokens\n",
    "    \"\"\"\n",
    "    if type(sentence)==str:\n",
    "        clean = clean_str(sentence)  ## cleaned sentence\n",
    "        tokens = clean.split()\n",
    "        return tokens\n",
    "    elif type(sentence)==list:\n",
    "        res = []\n",
    "        for it in sentence:\n",
    "            res.append(clean_str(it).split())\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_sentences = process_document(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "tk = Tokenizer(lower = True, num_words=max_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "               split=' ', char_level=False, oov_token='<unk>')\n",
    "tk.fit_on_texts(tk_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of questions in train is 65.\n",
      "Average word length of questions in test is 96.\n"
     ]
    }
   ],
   "source": [
    "print('Average word length of questions in train is {0:.0f}.'.format(np.mean(train['sentences'].apply(lambda x: len(clean_str(x).split())))))\n",
    "print('Average word length of questions in test is {0:.0f}.'.format(np.std(train['sentences'].apply(lambda x: len(clean_str(x).split())))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you both so much, I hope the very best for both you and your families. Tom '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_file='../glove.6B/glove.6B.100d.txt'\n",
    "def loadData_Tokenizer(X_train, X_test, MAX_NB_WORDS=75000,\n",
    "                       MAX_SEQUENCE_LENGTH=500, w2v_file=w2v_file):\n",
    "    \"\"\"\n",
    "    use glove embedding\n",
    "    \"\"\"\n",
    "    np.random.seed(7)\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post', truncating='post')\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    indices = np.arange(text.shape[0])\n",
    "    # np.random.shuffle(indices)\n",
    "    text = text[indices]\n",
    "    print(text.shape)\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    embeddings_index = {}\n",
    "    f = open(w2v_file, encoding=\"utf8\") ## GloVe file which could be download https://nlp.stanford.edu/projects/glove/\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    return (X_train, X_test, word_index,embeddings_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## construct the initialized embedding matrix based on pretrained matrix\n",
    "def init_embed(word_index, embeddings_index, emb_dim=100):\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, emb_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[i]) !=len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
    "                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
    "                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7623 unique tokens.\n",
      "(1756, 256)\n",
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "train_ids,test_ids,wd_ind,emb_ind=loadData_Tokenizer(X_train, X_test, MAX_NB_WORDS=10000,\n",
    "                       MAX_SEQUENCE_LENGTH=256, w2v_file=w2v_file)\n",
    "embedding_matrix=init_embed(wd_ind, emb_ind, emb_dim=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  18,    7,    0, ...,    0,    0,    0],\n",
       "       [  95,   60,   27, ...,    0,    0,    0],\n",
       "       [ 324,    4,  180, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [1423,  923,   11, ...,    0,    0,    0],\n",
       "       [  18,    7,    0, ...,    0,    0,    0],\n",
       "       [   1,   12,    6, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KIMCNN2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, label_size, max_seq_len,\n",
    "                 kernel_sizes, kernel_nums, keep_dropout, embedding_matrix):\n",
    "        super(KIMCNN2D,self).__init__()\n",
    "        self.embedding_dim = embedding_matrix.shape[1]\n",
    "        self.label_size = label_size\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.kernel_nums = kernel_nums        \n",
    "        self.keep_dropout = keep_dropout\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        #self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks] -- Ci, Co -- channels in or out\n",
    "        #self.convs1 = nn.ModuleList([nn.Conv2d(1, num, (size, self.embedding_dim)) for size,num in\n",
    "        #                             zip(self.kernel_sizes,self.kernel_nums)])\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(1, num, self.embedding_dim * size, \n",
    "                                              stride=self.embedding_dim) for size,num in \n",
    "                                    zip(self.kernel_sizes, self.kernel_nums)])\n",
    "\n",
    "        '''\n",
    "        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n",
    "        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n",
    "        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n",
    "        '''\n",
    "        self.dropout = nn.Dropout(self.keep_dropout)\n",
    "        self.fc = nn.Linear(sum(self.kernel_nums), self.label_size)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3) #(N,Co,W)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = self.embedding(inp).view(-1, 1, self.embedding_dim * self.max_seq_len)\n",
    "#         if self.embedding_type == \"multichannel\":\n",
    "#             x2 = self.embedding2(inp).view(-1, 1, self.embedding_dim * self.max_seq_len)\n",
    "#             x = torch.cat((x, x2), 1)\n",
    "        conv_results = [\n",
    "            F.max_pool1d(F.relu(self.convs[i](x)), self.max_seq_len - self.kernel_sizes[i] + 1)\n",
    "                .view(-1, self.kernel_nums[i])\n",
    "            for i in range(len(self.convs))]\n",
    "\n",
    "        x = torch.cat(conv_results, 1)\n",
    "        x = F.dropout(x, p=self.keep_dropout)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward2(self, x):\n",
    "        '''\n",
    "        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n",
    "        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n",
    "        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n",
    "        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n",
    "        '''\n",
    "        x = self.embedding(x) # (N,W,D)\n",
    "        x = x.unsqueeze(1) # (N,Ci,W,D)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x) # (N,len(Ks)*Co)\n",
    "        logit = self.fc(x) # (N,C)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import time,datetime\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluate the performance of current model\n",
    "def evaluate_model(clf_model, validation_dataloader, save_dir):\n",
    "    \"\"\"\n",
    "    :params[in]: clf_model, the pre-trained classifier\n",
    "    :params[in]: validation_dataloader, the validation dataset\n",
    "    :params[in]: save_dir, the directory name to save the fine-tuned model\n",
    "    \n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    clf_model.eval()\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    true_labels,pred_labels=[],[]\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = clf_model(b_input_ids)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        ## pred_labels/true_labels in a batch flatten\n",
    "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "        true_flat = label_ids.flatten()\n",
    "\n",
    "        # true labels and predicted labels\n",
    "        true_labels += true_flat.tolist()\n",
    "        pred_labels += pred_flat.tolist()\n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "    ## pdb check\n",
    "    #pdb.set_trace()\n",
    "    # Report the final accuracy for this validation run\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    #clf_model.save_pretrained(save_dir)  ## save model\n",
    "    print(classification_report(true_labels, pred_labels,digits=3))\n",
    "    print(classification_report(true_labels, pred_labels,digits=3),\n",
    "      file=open(save_dir+'/result.txt','w'))\n",
    "    print(\"  Accuracy: {0:.3f}\".format(eval_accuracy/nb_eval_steps),\n",
    "          file=open(save_dir+'/result.txt','w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(clf_model, train_dataloader, validation_dataloader, base_dir,\n",
    "               lr, epochs=4, eval_every_num_iters=40, seed_val = 42, weights= None):\n",
    "    \"\"\"train and evaluate a deep learning model\n",
    "    :params[in]: clf_model, a classifier\n",
    "    :params[in]: train_dataloader, training data\n",
    "    :params[in]: validation_dataloader, validation data\n",
    "    :params[in]: base_dir, output directory to create the directory to save results\n",
    "    :params[in]: lr, the learning rate\n",
    "    :params[in]: epochs, the number of training epochs\n",
    "    :params[in]: eval_every_num_iters, the number of iterations to evaluate\n",
    "    :params[in]: seed_val, set a random seed\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(clf_model.parameters(),\n",
    "                      lr = lr)\n",
    "    ## cross entropy loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Total number of training steps is number of batches * number of epochs.\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    # Create the learning rate scheduler. # gamma = decaying factor\n",
    "    scheduler = StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "    # see if weights is None:\n",
    "    if weights != None:\n",
    "        weights = torch.FloatTensor(weights)\n",
    "    # Set the seed value all over the place to make this reproducible.\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    \n",
    "    # Store the average loss after each epoch so we can plot them.\n",
    "    loss_values = []\n",
    "    \n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "        \n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "    \n",
    "        # Reset the total loss for this epoch.\n",
    "        total_loss = 0\n",
    "        ## print the learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(' learning rate is: ', param_group['lr'])\n",
    "        \n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    \n",
    "        # For each batch of training data...\n",
    "        for iters, batch in enumerate(train_dataloader):\n",
    "            clf_model.train()  ## model training mode\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_labels = batch[1].to(device)\n",
    "    \n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. PyTorch doesn't do this automatically because \n",
    "            # accumulating the gradients is \"convenient while training RNNs\". \n",
    "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "            clf_model.zero_grad()        \n",
    "    \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # This will return the loss (rather than the model output) because we\n",
    "            # have provided the `labels`.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = clf_model(b_input_ids)\n",
    "             \n",
    "            # The call to `model` always returns a tuple, so we need to pull the \n",
    "            # loss value out of the tuple.\n",
    "            loss = criterion(outputs, b_labels)\n",
    "    \n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "    \n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            optimizer.step()    \n",
    "            # eveluate the performance after some iterations\n",
    "            if iters % eval_every_num_iters == 0 and not iters == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(iters, len(train_dataloader), elapsed))\n",
    "                tmp_dir = base_dir+'/epoch'+str(epoch_i+1)+'iteration'+str(iters)\n",
    "                ## save pretrained model\n",
    "                evaluate_model(clf_model, validation_dataloader, tmp_dir)\n",
    "        # Update the learning rate each epoch\n",
    "        scheduler.step()\n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)            \n",
    "        #pdb.set_trace()\n",
    "        # Store the loss value for plotting the learning curve.\n",
    "        loss_values.append(avg_train_loss)\n",
    "        # save the data after epochs\n",
    "        tmp_dir = base_dir+'/epoch'+str(epoch_i+1)+'_done'\n",
    "        ## save pretrained model\n",
    "        evaluate_model(clf_model, validation_dataloader, tmp_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7623 unique tokens.\n",
      "(1756, 256)\n",
      "Total 400000 word vectors.\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.992     0.969     0.980       127\n",
      "           1      0.897     0.822     0.858       191\n",
      "           2      0.340     0.529     0.414        34\n",
      "\n",
      "    accuracy                          0.847       352\n",
      "   macro avg      0.743     0.773     0.751       352\n",
      "weighted avg      0.877     0.847     0.859       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.933     0.984     0.958       127\n",
      "           1      0.896     0.764     0.825       191\n",
      "           2      0.327     0.529     0.404        34\n",
      "\n",
      "    accuracy                          0.821       352\n",
      "   macro avg      0.719     0.759     0.729       352\n",
      "weighted avg      0.854     0.821     0.832       352\n",
      "\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.969     0.969       127\n",
      "           1      0.848     0.963     0.902       191\n",
      "           2      0.375     0.088     0.143        34\n",
      "\n",
      "    accuracy                          0.881       352\n",
      "   macro avg      0.730     0.673     0.671       352\n",
      "weighted avg      0.846     0.881     0.853       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.919     0.976     0.947       127\n",
      "           1      0.896     0.675     0.770       191\n",
      "           2      0.260     0.559     0.355        34\n",
      "\n",
      "    accuracy                          0.773       352\n",
      "   macro avg      0.692     0.737     0.691       352\n",
      "weighted avg      0.843     0.773     0.794       352\n",
      "\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.945     0.972       127\n",
      "           1      0.867     0.890     0.879       191\n",
      "           2      0.333     0.353     0.343        34\n",
      "\n",
      "    accuracy                          0.858       352\n",
      "   macro avg      0.734     0.729     0.731       352\n",
      "weighted avg      0.864     0.858     0.860       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.992     0.945     0.968       127\n",
      "           1      0.858     0.916     0.886       191\n",
      "           2      0.444     0.353     0.393        34\n",
      "\n",
      "    accuracy                          0.872       352\n",
      "   macro avg      0.765     0.738     0.749       352\n",
      "weighted avg      0.866     0.872     0.868       352\n",
      "\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.992     0.937     0.964       127\n",
      "           1      0.826     0.921     0.871       191\n",
      "           2      0.158     0.088     0.113        34\n",
      "\n",
      "    accuracy                          0.847       352\n",
      "   macro avg      0.659     0.649     0.649       352\n",
      "weighted avg      0.821     0.847     0.831       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.992     0.969     0.980       127\n",
      "           1      0.905     0.696     0.787       191\n",
      "           2      0.259     0.618     0.365        34\n",
      "\n",
      "    accuracy                          0.787       352\n",
      "   macro avg      0.719     0.761     0.711       352\n",
      "weighted avg      0.874     0.787     0.816       352\n",
      "\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.0025\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.976     0.976       127\n",
      "           1      0.879     0.874     0.877       191\n",
      "           2      0.371     0.382     0.377        34\n",
      "\n",
      "    accuracy                          0.864       352\n",
      "   macro avg      0.742     0.744     0.743       352\n",
      "weighted avg      0.865     0.864     0.864       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.976     0.965       127\n",
      "           1      0.877     0.895     0.886       191\n",
      "           2      0.407     0.324     0.361        34\n",
      "\n",
      "    accuracy                          0.869       352\n",
      "   macro avg      0.746     0.732     0.737       352\n",
      "weighted avg      0.859     0.869     0.864       352\n",
      "\n",
      "Found 7623 unique tokens.\n",
      "(1756, 256)\n",
      "Total 400000 word vectors.\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:16.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.976     0.973       127\n",
      "           1      0.879     0.916     0.897       191\n",
      "           2      0.440     0.324     0.373        34\n",
      "\n",
      "    accuracy                          0.881       352\n",
      "   macro avg      0.763     0.739     0.748       352\n",
      "weighted avg      0.869     0.881     0.874       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.984     0.977       127\n",
      "           1      0.868     0.864     0.866       191\n",
      "           2      0.303     0.294     0.299        34\n",
      "\n",
      "    accuracy                          0.852       352\n",
      "   macro avg      0.713     0.714     0.714       352\n",
      "weighted avg      0.850     0.852     0.851       352\n",
      "\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.955     0.992     0.973       127\n",
      "           1      0.847     0.958     0.899       191\n",
      "           2      0.250     0.029     0.053        34\n",
      "\n",
      "    accuracy                          0.881       352\n",
      "   macro avg      0.684     0.660     0.642       352\n",
      "weighted avg      0.828     0.881     0.844       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.843     0.915       127\n",
      "           1      0.835     0.901     0.866       191\n",
      "           2      0.282     0.324     0.301        34\n",
      "\n",
      "    accuracy                          0.824       352\n",
      "   macro avg      0.706     0.689     0.694       352\n",
      "weighted avg      0.841     0.824     0.829       352\n",
      "\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.912     0.976     0.943       127\n",
      "           1      0.884     0.838     0.860       191\n",
      "           2      0.371     0.382     0.377        34\n",
      "\n",
      "    accuracy                          0.844       352\n",
      "   macro avg      0.722     0.732     0.727       352\n",
      "weighted avg      0.844     0.844     0.843       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.940     0.992     0.966       127\n",
      "           1      0.902     0.822     0.860       191\n",
      "           2      0.386     0.500     0.436        34\n",
      "\n",
      "    accuracy                          0.852       352\n",
      "   macro avg      0.743     0.771     0.754       352\n",
      "weighted avg      0.866     0.852     0.857       352\n",
      "\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:16.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.945     0.972       127\n",
      "           1      0.876     0.890     0.883       191\n",
      "           2      0.368     0.412     0.389        34\n",
      "\n",
      "    accuracy                          0.864       352\n",
      "   macro avg      0.748     0.749     0.748       352\n",
      "weighted avg      0.872     0.864     0.867       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.969     0.972       127\n",
      "           1      0.910     0.843     0.875       191\n",
      "           2      0.388     0.559     0.458        34\n",
      "\n",
      "    accuracy                          0.861       352\n",
      "   macro avg      0.758     0.790     0.768       352\n",
      "weighted avg      0.883     0.861     0.870       352\n",
      "\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.0025\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.961     0.961     0.961       127\n",
      "           1      0.874     0.869     0.871       191\n",
      "           2      0.371     0.382     0.377        34\n",
      "\n",
      "    accuracy                          0.855       352\n",
      "   macro avg      0.735     0.737     0.736       352\n",
      "weighted avg      0.857     0.855     0.856       352\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.945     0.953     0.949       127\n",
      "           1      0.875     0.880     0.877       191\n",
      "           2      0.375     0.353     0.364        34\n",
      "\n",
      "    accuracy                          0.855       352\n",
      "   macro avg      0.732     0.728     0.730       352\n",
      "weighted avg      0.852     0.855     0.854       352\n",
      "\n",
      "Found 7623 unique tokens.\n",
      "(1756, 256)\n",
      "Total 400000 word vectors.\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.890     0.942       127\n",
      "           1      0.799     1.000     0.888       191\n",
      "           2      0.000     0.000     0.000        34\n",
      "\n",
      "    accuracy                          0.864       352\n",
      "   macro avg      0.600     0.630     0.610       352\n",
      "weighted avg      0.794     0.864     0.822       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.918     0.969     0.943       127\n",
      "           1      0.861     0.812     0.836       191\n",
      "           2      0.289     0.324     0.306        34\n",
      "\n",
      "    accuracy                          0.821       352\n",
      "   macro avg      0.689     0.701     0.695       352\n",
      "weighted avg      0.826     0.821     0.823       352\n",
      "\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.919     0.976     0.947       127\n",
      "           1      0.940     0.660     0.775       191\n",
      "           2      0.313     0.765     0.444        34\n",
      "\n",
      "    accuracy                          0.784       352\n",
      "   macro avg      0.724     0.800     0.722       352\n",
      "weighted avg      0.872     0.784     0.805       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.932     0.976     0.954       127\n",
      "           1      0.859     0.832     0.846       191\n",
      "           2      0.324     0.324     0.324        34\n",
      "\n",
      "    accuracy                          0.835       352\n",
      "   macro avg      0.705     0.711     0.708       352\n",
      "weighted avg      0.834     0.835     0.834       352\n",
      "\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.976     0.976       127\n",
      "           1      0.848     0.937     0.891       191\n",
      "           2      0.357     0.147     0.208        34\n",
      "\n",
      "    accuracy                          0.875       352\n",
      "   macro avg      0.727     0.687     0.692       352\n",
      "weighted avg      0.847     0.875     0.856       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.953     0.964       127\n",
      "           1      0.902     0.864     0.882       191\n",
      "           2      0.467     0.618     0.532        34\n",
      "\n",
      "    accuracy                          0.872       352\n",
      "   macro avg      0.781     0.811     0.793       352\n",
      "weighted avg      0.886     0.872     0.878       352\n",
      "\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:22.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.947     0.976     0.961       127\n",
      "           1      0.877     0.895     0.886       191\n",
      "           2      0.423     0.324     0.367        34\n",
      "\n",
      "    accuracy                          0.869       352\n",
      "   macro avg      0.749     0.732     0.738       352\n",
      "weighted avg      0.858     0.869     0.863       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.961     0.969     0.965       127\n",
      "           1      0.860     0.869     0.865       191\n",
      "           2      0.355     0.324     0.338        34\n",
      "\n",
      "    accuracy                          0.852       352\n",
      "   macro avg      0.725     0.720     0.723       352\n",
      "weighted avg      0.848     0.852     0.850       352\n",
      "\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.0025\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.961     0.961     0.961       127\n",
      "           1      0.856     0.937     0.895       191\n",
      "           2      0.500     0.235     0.320        34\n",
      "\n",
      "    accuracy                          0.878       352\n",
      "   macro avg      0.772     0.711     0.725       352\n",
      "weighted avg      0.860     0.878     0.863       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.968     0.961     0.964       127\n",
      "           1      0.862     0.948     0.903       191\n",
      "           2      0.625     0.294     0.400        34\n",
      "\n",
      "    accuracy                          0.889       352\n",
      "   macro avg      0.818     0.734     0.756       352\n",
      "weighted avg      0.877     0.889     0.876       352\n",
      "\n",
      "Found 7623 unique tokens.\n",
      "(1756, 256)\n",
      "Total 400000 word vectors.\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:14.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.947     0.984     0.965       127\n",
      "           1      0.950     0.503     0.658       191\n",
      "           2      0.244     0.853     0.379        34\n",
      "\n",
      "    accuracy                          0.710       352\n",
      "   macro avg      0.714     0.780     0.667       352\n",
      "weighted avg      0.881     0.710     0.742       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.924     0.961     0.942       127\n",
      "           1      0.914     0.775     0.839       191\n",
      "           2      0.379     0.647     0.478        34\n",
      "\n",
      "    accuracy                          0.830       352\n",
      "   macro avg      0.739     0.794     0.753       352\n",
      "weighted avg      0.866     0.830     0.841       352\n",
      "\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.912     0.984     0.947       127\n",
      "           1      0.874     0.838     0.856       191\n",
      "           2      0.375     0.353     0.364        34\n",
      "\n",
      "    accuracy                          0.844       352\n",
      "   macro avg      0.721     0.725     0.722       352\n",
      "weighted avg      0.840     0.844     0.841       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.866     0.928       127\n",
      "           1      0.800     0.942     0.865       191\n",
      "           2      0.235     0.118     0.157        34\n",
      "\n",
      "    accuracy                          0.835       352\n",
      "   macro avg      0.678     0.642     0.650       352\n",
      "weighted avg      0.818     0.835     0.820       352\n",
      "\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.984     0.969       127\n",
      "           1      0.878     0.901     0.889       191\n",
      "           2      0.440     0.324     0.373        34\n",
      "\n",
      "    accuracy                          0.875       352\n",
      "   macro avg      0.757     0.736     0.744       352\n",
      "weighted avg      0.863     0.875     0.868       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.984     0.977       127\n",
      "           1      0.871     0.916     0.893       191\n",
      "           2      0.409     0.265     0.321        34\n",
      "\n",
      "    accuracy                          0.878       352\n",
      "   macro avg      0.750     0.722     0.730       352\n",
      "weighted avg      0.862     0.878     0.868       352\n",
      "\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.977     0.984     0.980       127\n",
      "           1      0.888     0.874     0.881       191\n",
      "           2      0.389     0.412     0.400        34\n",
      "\n",
      "    accuracy                          0.869       352\n",
      "   macro avg      0.751     0.757     0.754       352\n",
      "weighted avg      0.872     0.869     0.871       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.947     0.984     0.965       127\n",
      "           1      0.883     0.832     0.857       191\n",
      "           2      0.350     0.412     0.378        34\n",
      "\n",
      "    accuracy                          0.847       352\n",
      "   macro avg      0.727     0.743     0.734       352\n",
      "weighted avg      0.855     0.847     0.850       352\n",
      "\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   160  of    176.    Elapsed: 0:00:16.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.976     0.976       127\n",
      "           1      0.886     0.932     0.908       191\n",
      "           2      0.542     0.382     0.448        34\n",
      "\n",
      "    accuracy                          0.895       352\n",
      "   macro avg      0.801     0.764     0.778       352\n",
      "weighted avg      0.885     0.895     0.888       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.961     0.969     0.965       127\n",
      "           1      0.915     0.848     0.880       191\n",
      "           2      0.447     0.618     0.519        34\n",
      "\n",
      "    accuracy                          0.869       352\n",
      "   macro avg      0.774     0.811     0.788       352\n",
      "weighted avg      0.886     0.869     0.876       352\n",
      "\n",
      "Found 7623 unique tokens.\n",
      "(1756, 256)\n",
      "Total 400000 word vectors.\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:14.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.906     0.992     0.947       127\n",
      "           1      0.899     0.749     0.817       191\n",
      "           2      0.296     0.471     0.364        34\n",
      "\n",
      "    accuracy                          0.810       352\n",
      "   macro avg      0.701     0.737     0.709       352\n",
      "weighted avg      0.844     0.810     0.820       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.961     0.976     0.969       127\n",
      "           1      0.837     0.969     0.898       191\n",
      "           2      0.000     0.000     0.000        34\n",
      "\n",
      "    accuracy                          0.878       352\n",
      "   macro avg      0.599     0.648     0.622       352\n",
      "weighted avg      0.801     0.878     0.837       352\n",
      "\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.786     0.984     0.874       127\n",
      "           1      0.940     0.492     0.646       191\n",
      "           2      0.247     0.676     0.362        34\n",
      "\n",
      "    accuracy                          0.688       352\n",
      "   macro avg      0.658     0.718     0.627       352\n",
      "weighted avg      0.818     0.688     0.701       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.890     0.942       127\n",
      "           1      0.874     0.728     0.794       191\n",
      "           2      0.287     0.676     0.404        34\n",
      "\n",
      "    accuracy                          0.781       352\n",
      "   macro avg      0.721     0.765     0.713       352\n",
      "weighted avg      0.863     0.781     0.810       352\n",
      "\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.969     0.972       127\n",
      "           1      0.887     0.906     0.896       191\n",
      "           2      0.484     0.441     0.462        34\n",
      "\n",
      "    accuracy                          0.884       352\n",
      "   macro avg      0.782     0.772     0.777       352\n",
      "weighted avg      0.880     0.884     0.882       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.976     0.976       127\n",
      "           1      0.907     0.869     0.888       191\n",
      "           2      0.429     0.529     0.474        34\n",
      "\n",
      "    accuracy                          0.875       352\n",
      "   macro avg      0.771     0.792     0.779       352\n",
      "weighted avg      0.886     0.875     0.880       352\n",
      "\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:16.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.925     0.969     0.946       127\n",
      "           1      0.895     0.890     0.892       191\n",
      "           2      0.483     0.412     0.444        34\n",
      "\n",
      "    accuracy                          0.872       352\n",
      "   macro avg      0.767     0.757     0.761       352\n",
      "weighted avg      0.866     0.872     0.869       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.969     0.972       127\n",
      "           1      0.919     0.827     0.871       191\n",
      "           2      0.370     0.588     0.455        34\n",
      "\n",
      "    accuracy                          0.855       352\n",
      "   macro avg      0.755     0.795     0.766       352\n",
      "weighted avg      0.886     0.855     0.867       352\n",
      "\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.0025\n",
      "  Batch   160  of    176.    Elapsed: 0:00:15.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.961     0.968       127\n",
      "           1      0.889     0.921     0.905       191\n",
      "           2      0.552     0.471     0.508        34\n",
      "\n",
      "    accuracy                          0.892       352\n",
      "   macro avg      0.806     0.784     0.794       352\n",
      "weighted avg      0.888     0.892     0.889       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.976     0.988       127\n",
      "           1      0.875     0.953     0.912       191\n",
      "           2      0.500     0.294     0.370        34\n",
      "\n",
      "    accuracy                          0.898       352\n",
      "   macro avg      0.792     0.741     0.757       352\n",
      "weighted avg      0.884     0.898     0.887       352\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    import os\n",
    "    for seed in [42,52, 62, 72, 82]:\n",
    "        batch_size = 8\n",
    "        train_ids,test_ids,wd_ind,emb_ind=loadData_Tokenizer(X_train, X_test, MAX_NB_WORDS=10000,\n",
    "                       MAX_SEQUENCE_LENGTH=256, w2v_file=w2v_file)\n",
    "        embedding_matrix=init_embed(wd_ind, emb_ind, emb_dim=100)\n",
    "        ## initialize a classifier\n",
    "        clf_model=KIMCNN2D(label_size=3, kernel_sizes=[1,2,3,4,5], keep_dropout=.5,max_seq_len=256,\n",
    "                  kernel_nums=[200, 300, 500, 500,200], embedding_matrix=torch.FloatTensor(embedding_matrix))\n",
    "        # Create the DataLoader for our training set.\n",
    "        train_data = TensorDataset(torch.LongTensor(train_ids), torch.tensor(y_train))\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "        # Create the DataLoader for our validation set.\n",
    "        validation_data = TensorDataset(torch.LongTensor(test_ids), torch.tensor(y_test))\n",
    "        validation_sampler = SequentialSampler(validation_data)\n",
    "        validation_dataloader = DataLoader(validation_data, sampler=validation_sampler,\n",
    "                                           batch_size=batch_size)\n",
    "        base_dir = 'textCNN/CNN_seed'+str(seed)\n",
    "        train_eval(clf_model, train_dataloader, validation_dataloader, base_dir, \\\n",
    "            lr=1.0e-2, epochs=5, eval_every_num_iters=160, seed_val = seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = np.array([[0.757,0.792,0.741,0.898],\n",
    "               [0.788,0.774, 0.811,0.869],\n",
    "                [0.756,0.818,0.734,0.889],\n",
    "                [0.730,0.732,0.728,0.855],\n",
    "                [0.737,0.746,0.732,0.869]\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7536, 0.7724, 0.7492, 0.876 ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(res, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02016532, 0.03096837, 0.03118589, 0.01544021])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(res, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194  , -0.24487001,  0.72812003, -0.39961001,  0.083172  ,\n",
       "        0.043953  , -0.39140999,  0.3344    , -0.57545   ,  0.087459  ,\n",
       "        0.28786999, -0.06731   ,  0.30906001, -0.26383999, -0.13231   ,\n",
       "       -0.20757   ,  0.33395001, -0.33848   , -0.31742999, -0.48335999,\n",
       "        0.1464    , -0.37303999,  0.34577   ,  0.052041  ,  0.44946   ,\n",
       "       -0.46970999,  0.02628   , -0.54154998, -0.15518001, -0.14106999,\n",
       "       -0.039722  ,  0.28277001,  0.14393   ,  0.23464   , -0.31020999,\n",
       "        0.086173  ,  0.20397   ,  0.52623999,  0.17163999, -0.082378  ,\n",
       "       -0.71787   , -0.41531   ,  0.20334999, -0.12763   ,  0.41367   ,\n",
       "        0.55186999,  0.57907999, -0.33476999, -0.36559001, -0.54856998,\n",
       "       -0.062892  ,  0.26583999,  0.30204999,  0.99774998, -0.80480999,\n",
       "       -3.0243001 ,  0.01254   , -0.36941999,  2.21670008,  0.72201002,\n",
       "       -0.24978   ,  0.92136002,  0.034514  ,  0.46744999,  1.10790002,\n",
       "       -0.19358   , -0.074575  ,  0.23353   , -0.052062  , -0.22044   ,\n",
       "        0.057162  , -0.15806   , -0.30798   , -0.41624999,  0.37972   ,\n",
       "        0.15006   , -0.53211999, -0.20550001, -1.25259995,  0.071624  ,\n",
       "        0.70564997,  0.49744001, -0.42063001,  0.26148   , -1.53799999,\n",
       "       -0.30223   , -0.073438  , -0.28312001,  0.37103999, -0.25217   ,\n",
       "        0.016215  , -0.017099  , -0.38984001,  0.87423998, -0.72569001,\n",
       "       -0.51058   , -0.52028   , -0.1459    ,  0.82779998,  0.27061999])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "## load data from json\n",
    "with open('original_labelled_data.json', 'r') as jf:\n",
    "    data=json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences,labels=data['sentences'], data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sentences, labels, test_size=0.2, random_state=2020, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# set up fields\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = string.lower()    ## lower-wise case\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()\n",
    "\n",
    "def process_document(sentence):\n",
    "    \"\"\"\n",
    "    process a document \n",
    "    :params[in]: sentence, a string or list of strings\n",
    "    :params[out]: tokens, list of tokens\n",
    "    \"\"\"\n",
    "    if type(sentence)==str:\n",
    "        clean = clean_str(sentence)  ## cleaned sentence\n",
    "        tokens = clean.split()\n",
    "        return tokens\n",
    "    elif type(sentence)==list:\n",
    "        res = []\n",
    "        for it in sentence:\n",
    "            res.append(clean_str(it).split())\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_sentences = process_document(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "tk = Tokenizer(lower = True, num_words=max_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "               split=' ', char_level=False, oov_token='<unk>')\n",
    "tk.fit_on_texts(tk_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of questions in train is 65.\n",
      "Average word length of questions in test is 96.\n"
     ]
    }
   ],
   "source": [
    "print('Average word length of questions in train is {0:.0f}.'.format(np.mean(train['sentences'].apply(lambda x: len(clean_str(x).split())))))\n",
    "print('Average word length of questions in test is {0:.0f}.'.format(np.std(train['sentences'].apply(lambda x: len(clean_str(x).split())))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you both so much, I hope the very best for both you and your families. Tom '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_file='../glove.6B/glove.6B.100d.txt'\n",
    "def loadData_Tokenizer(X_train, X_test, MAX_NB_WORDS=75000,\n",
    "                       MAX_SEQUENCE_LENGTH=500, w2v_file=w2v_file):\n",
    "    \"\"\"\n",
    "    use glove embedding\n",
    "    \"\"\"\n",
    "    np.random.seed(7)\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post', truncating='post')\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    indices = np.arange(text.shape[0])\n",
    "    # np.random.shuffle(indices)\n",
    "    text = text[indices]\n",
    "    print(text.shape)\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    embeddings_index = {}\n",
    "    f = open(w2v_file, encoding=\"utf8\") ## GloVe file which could be download https://nlp.stanford.edu/projects/glove/\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    return (X_train, X_test, word_index,embeddings_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## construct the initialized embedding matrix based on pretrained matrix\n",
    "def init_embed(word_index, embeddings_index, emb_dim=100):\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, emb_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[i]) !=len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
    "                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
    "                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7623 unique tokens.\n",
      "(1756, 256)\n",
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "train_ids,test_ids,wd_ind,emb_ind=loadData_Tokenizer(X_train, X_test, MAX_NB_WORDS=10000,\n",
    "                       MAX_SEQUENCE_LENGTH=256, w2v_file=w2v_file)\n",
    "embedding_matrix=init_embed(wd_ind, emb_ind, emb_dim=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10//2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttention(torch.nn.Module):\n",
    "    def __init__(self, label_size, hidden_dim, batch_size, embedding_matrix,\n",
    "                keep_dropout=.5, lstm_layers=1):\n",
    "\n",
    "        super(LSTMAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.use_gpu = torch.cuda.is_available()\n",
    "        self.embedding_dim = embedding_matrix.shape[1]\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.num_layers = lstm_layers\n",
    "        self.dropout = keep_dropout\n",
    "        self.bilstm = nn.LSTM(self.embedding_dim, hidden_dim // 2, batch_first=True, num_layers=self.num_layers,\n",
    "                              dropout=self.dropout, bidirectional=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        ##self.mean = opt.__dict__.get(\"lstm_mean\",True) -- no use in the class\n",
    "        self.attn_fc = torch.nn.Linear(self.embedding_dim, 1)\n",
    "\n",
    "    def init_hidden(self, batch_size=None):\n",
    "        \"\"\"\n",
    "        initialize hidden state and cell state\n",
    "        \"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size= self.batch_size\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            h0 = Variable(torch.zeros(2*self.num_layers, batch_size, self.hidden_dim // 2).cuda())\n",
    "            c0 = Variable(torch.zeros(2*self.num_layers, batch_size, self.hidden_dim // 2).cuda())\n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(2*self.num_layers, batch_size, self.hidden_dim // 2))\n",
    "            c0 = Variable(torch.zeros(2*self.num_layers, batch_size, self.hidden_dim // 2))\n",
    "        return (h0, c0)\n",
    "\n",
    "\n",
    "    def attention(self, rnn_out, state):\n",
    "        merged_state = torch.cat([s for s in state],1)\n",
    "        merged_state = merged_state.squeeze(0).unsqueeze(2)\n",
    "        # (batch, seq_len, cell_size) * (batch, cell_size, 1) = (batch, seq_len, 1)\n",
    "        weights = torch.bmm(rnn_out, merged_state)\n",
    "        weights = torch.nn.functional.softmax(weights.squeeze(2)).unsqueeze(2)\n",
    "        # (batch, cell_size, seq_len) * (batch, seq_len, 1) = (batch, cell_size, 1)\n",
    "        return torch.bmm(torch.transpose(rnn_out, 1, 2), weights).squeeze(2)\n",
    "    \n",
    "\n",
    "    def forward(self, X):\n",
    "        embedded = self.word_embeddings(X)\n",
    "        hidden= self.init_hidden(X.size()[0]) \n",
    "        rnn_out, hidden = self.bilstm(embedded, hidden)\n",
    "        h_n, c_n = hidden\n",
    "        attn_out = self.attention(rnn_out, h_n)\n",
    "        logits = self.hidden2label(attn_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import time,datetime\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluate the performance of current model\n",
    "def evaluate_model(clf_model, validation_dataloader, save_dir):\n",
    "    \"\"\"\n",
    "    :params[in]: clf_model, the pre-trained classifier\n",
    "    :params[in]: validation_dataloader, the validation dataset\n",
    "    :params[in]: save_dir, the directory name to save the fine-tuned model\n",
    "    \n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    clf_model.eval()\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    true_labels,pred_labels=[],[]\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = clf_model(b_input_ids)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        ## pred_labels/true_labels in a batch flatten\n",
    "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "        true_flat = label_ids.flatten()\n",
    "\n",
    "        # true labels and predicted labels\n",
    "        true_labels += true_flat.tolist()\n",
    "        pred_labels += pred_flat.tolist()\n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "    ## pdb check\n",
    "    #pdb.set_trace()\n",
    "    # Report the final accuracy for this validation run\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    #clf_model.save_pretrained(save_dir)  ## save model\n",
    "    print(classification_report(true_labels, pred_labels,digits=3))\n",
    "    print(classification_report(true_labels, pred_labels,digits=3),\n",
    "      file=open(save_dir+'/result.txt','w'))\n",
    "    print(\"  Accuracy: {0:.3f}\".format(eval_accuracy/nb_eval_steps),\n",
    "          file=open(save_dir+'/result.txt','w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(clf_model, train_dataloader, validation_dataloader, base_dir,\n",
    "               lr, epochs=4, eval_every_num_iters=40, seed_val = 42, weights= None):\n",
    "    \"\"\"train and evaluate a deep learning model\n",
    "    :params[in]: clf_model, a classifier\n",
    "    :params[in]: train_dataloader, training data\n",
    "    :params[in]: validation_dataloader, validation data\n",
    "    :params[in]: base_dir, output directory to create the directory to save results\n",
    "    :params[in]: lr, the learning rate\n",
    "    :params[in]: epochs, the number of training epochs\n",
    "    :params[in]: eval_every_num_iters, the number of iterations to evaluate\n",
    "    :params[in]: seed_val, set a random seed\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(clf_model.parameters(),\n",
    "                      lr = lr)\n",
    "    ## cross entropy loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Total number of training steps is number of batches * number of epochs.\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    # Create the learning rate scheduler. # gamma = decaying factor\n",
    "    scheduler = StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "    # see if weights is None:\n",
    "    if weights != None:\n",
    "        weights = torch.FloatTensor(weights)\n",
    "    # Set the seed value all over the place to make this reproducible.\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    \n",
    "    # Store the average loss after each epoch so we can plot them.\n",
    "    loss_values = []\n",
    "    \n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "        \n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "    \n",
    "        # Reset the total loss for this epoch.\n",
    "        total_loss = 0\n",
    "        ## print the learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(' learning rate is: ', param_group['lr'])\n",
    "        \n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    \n",
    "        # For each batch of training data...\n",
    "        for iters, batch in enumerate(train_dataloader):\n",
    "            clf_model.train()  ## model training mode\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_labels = batch[1].to(device)\n",
    "    \n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. PyTorch doesn't do this automatically because \n",
    "            # accumulating the gradients is \"convenient while training RNNs\". \n",
    "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "            clf_model.zero_grad()        \n",
    "    \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # This will return the loss (rather than the model output) because we\n",
    "            # have provided the `labels`.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = clf_model(b_input_ids)\n",
    "             \n",
    "            # The call to `model` always returns a tuple, so we need to pull the \n",
    "            # loss value out of the tuple.\n",
    "            loss = criterion(outputs, b_labels)\n",
    "    \n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "    \n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            # torch.nn.utils.clip_grad_norm_(clf_model.parameters(), 1.0)\n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()    \n",
    "            # eveluate the performance after some iterations\n",
    "            if iters % eval_every_num_iters == 0 and not iters == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(iters, len(train_dataloader), elapsed))\n",
    "                tmp_dir = base_dir+'/epoch'+str(epoch_i+1)+'iteration'+str(iters)\n",
    "                ## save pretrained model\n",
    "                evaluate_model(clf_model, validation_dataloader, tmp_dir)\n",
    "        # Update the learning rate each epoch\n",
    "        scheduler.step()\n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)            \n",
    "        #pdb.set_trace()\n",
    "        # Store the loss value for plotting the learning curve.\n",
    "        loss_values.append(avg_train_loss)\n",
    "        # save the data after epochs\n",
    "        tmp_dir = base_dir+'/epoch'+str(epoch_i+1)+'_done'\n",
    "        ## save pretrained model\n",
    "        evaluate_model(clf_model, validation_dataloader, tmp_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7623 unique tokens.\n",
      "(1756, 256)\n",
      "Total 400000 word vectors.\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DHE/ss1043/.local/lib/python3.5/site-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   160  of    176.    Elapsed: 0:00:44.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DHE/ss1043/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.961     0.976     0.969       127\n",
      "           1      0.834     0.974     0.899       191\n",
      "           2      0.000     0.000     0.000        34\n",
      "\n",
      "    accuracy                          0.881       352\n",
      "   macro avg      0.598     0.650     0.622       352\n",
      "weighted avg      0.799     0.881     0.837       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.969     0.972       127\n",
      "           1      0.832     0.984     0.902       191\n",
      "           2      0.000     0.000     0.000        34\n",
      "\n",
      "    accuracy                          0.884       352\n",
      "   macro avg      0.603     0.651     0.625       352\n",
      "weighted avg      0.804     0.884     0.840       352\n",
      "\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:45.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.940     0.984     0.962       127\n",
      "           1      0.858     0.916     0.886       191\n",
      "           2      0.400     0.176     0.245        34\n",
      "\n",
      "    accuracy                          0.869       352\n",
      "   macro avg      0.733     0.692     0.698       352\n",
      "weighted avg      0.843     0.869     0.851       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.940     0.984     0.962       127\n",
      "           1      0.874     0.838     0.856       191\n",
      "           2      0.333     0.353     0.343        34\n",
      "\n",
      "    accuracy                          0.844       352\n",
      "   macro avg      0.716     0.725     0.720       352\n",
      "weighted avg      0.846     0.844     0.844       352\n",
      "\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:45.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.976     0.973       127\n",
      "           1      0.884     0.874     0.879       191\n",
      "           2      0.400     0.412     0.406        34\n",
      "\n",
      "    accuracy                          0.866       352\n",
      "   macro avg      0.751     0.754     0.752       352\n",
      "weighted avg      0.868     0.866     0.867       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.976     0.973       127\n",
      "           1      0.886     0.853     0.869       191\n",
      "           2      0.375     0.441     0.405        34\n",
      "\n",
      "    accuracy                          0.858       352\n",
      "   macro avg      0.743     0.757     0.749       352\n",
      "weighted avg      0.866     0.858     0.862       352\n",
      "\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:45.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.962     0.984     0.973       127\n",
      "           1      0.872     0.895     0.884       191\n",
      "           2      0.385     0.294     0.333        34\n",
      "\n",
      "    accuracy                          0.869       352\n",
      "   macro avg      0.740     0.725     0.730       352\n",
      "weighted avg      0.857     0.869     0.863       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.984     0.969       127\n",
      "           1      0.879     0.874     0.877       191\n",
      "           2      0.387     0.353     0.369        34\n",
      "\n",
      "    accuracy                          0.864       352\n",
      "   macro avg      0.740     0.737     0.738       352\n",
      "weighted avg      0.859     0.864     0.861       352\n",
      "\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.0025\n",
      "  Batch   160  of    176.    Elapsed: 0:00:43.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.940     0.984     0.962       127\n",
      "           1      0.878     0.832     0.855       191\n",
      "           2      0.342     0.382     0.361        34\n",
      "\n",
      "    accuracy                          0.844       352\n",
      "   macro avg      0.720     0.733     0.726       352\n",
      "weighted avg      0.849     0.844     0.846       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.962     0.984     0.973       127\n",
      "           1      0.878     0.864     0.871       191\n",
      "           2      0.353     0.353     0.353        34\n",
      "\n",
      "    accuracy                          0.858       352\n",
      "   macro avg      0.731     0.734     0.732       352\n",
      "weighted avg      0.857     0.858     0.858       352\n",
      "\n",
      "Found 7623 unique tokens.\n",
      "(1756, 256)\n",
      "Total 400000 word vectors.\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:43.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.925     0.976     0.950       127\n",
      "           1      0.830     0.948     0.885       191\n",
      "           2      0.000     0.000     0.000        34\n",
      "\n",
      "    accuracy                          0.866       352\n",
      "   macro avg      0.585     0.641     0.612       352\n",
      "weighted avg      0.784     0.866     0.823       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.976     0.973       127\n",
      "           1      0.834     0.974     0.899       191\n",
      "           2      0.000     0.000     0.000        34\n",
      "\n",
      "    accuracy                          0.881       352\n",
      "   macro avg      0.601     0.650     0.624       352\n",
      "weighted avg      0.802     0.881     0.838       352\n",
      "\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:43.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.984     0.977       127\n",
      "           1      0.858     0.948     0.900       191\n",
      "           2      0.417     0.147     0.217        34\n",
      "\n",
      "    accuracy                          0.884       352\n",
      "   macro avg      0.748     0.693     0.698       352\n",
      "weighted avg      0.855     0.884     0.862       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.984     0.977       127\n",
      "           1      0.895     0.895     0.895       191\n",
      "           2      0.469     0.441     0.455        34\n",
      "\n",
      "    accuracy                          0.884       352\n",
      "   macro avg      0.778     0.774     0.775       352\n",
      "weighted avg      0.881     0.884     0.882       352\n",
      "\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:43.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.962     0.984     0.973       127\n",
      "           1      0.920     0.780     0.844       191\n",
      "           2      0.350     0.618     0.447        34\n",
      "\n",
      "    accuracy                          0.838       352\n",
      "   macro avg      0.744     0.794     0.755       352\n",
      "weighted avg      0.880     0.838     0.852       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.955     0.992     0.973       127\n",
      "           1      0.895     0.890     0.892       191\n",
      "           2      0.467     0.412     0.437        34\n",
      "\n",
      "    accuracy                          0.881       352\n",
      "   macro avg      0.772     0.765     0.768       352\n",
      "weighted avg      0.875     0.881     0.878       352\n",
      "\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:43.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.992     0.981       127\n",
      "           1      0.897     0.869     0.883       191\n",
      "           2      0.405     0.441     0.423        34\n",
      "\n",
      "    accuracy                          0.872       352\n",
      "   macro avg      0.757     0.767     0.762       352\n",
      "weighted avg      0.876     0.872     0.874       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.955     0.992     0.973       127\n",
      "           1      0.894     0.880     0.887       191\n",
      "           2      0.438     0.412     0.424        34\n",
      "\n",
      "    accuracy                          0.875       352\n",
      "   macro avg      0.762     0.761     0.761       352\n",
      "weighted avg      0.872     0.875     0.873       352\n",
      "\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.0025\n",
      "  Batch   160  of    176.    Elapsed: 0:00:43.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.955     0.992     0.973       127\n",
      "           1      0.904     0.843     0.873       191\n",
      "           2      0.405     0.500     0.447        34\n",
      "\n",
      "    accuracy                          0.864       352\n",
      "   macro avg      0.755     0.778     0.764       352\n",
      "weighted avg      0.874     0.864     0.868       352\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.955     0.992     0.973       127\n",
      "           1      0.894     0.880     0.887       191\n",
      "           2      0.438     0.412     0.424        34\n",
      "\n",
      "    accuracy                          0.875       352\n",
      "   macro avg      0.762     0.761     0.761       352\n",
      "weighted avg      0.872     0.875     0.873       352\n",
      "\n",
      "Found 7623 unique tokens.\n",
      "(1756, 256)\n",
      "Total 400000 word vectors.\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:44.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.953     0.964       127\n",
      "           1      0.825     0.984     0.897       191\n",
      "           2      0.000     0.000     0.000        34\n",
      "\n",
      "    accuracy                          0.878       352\n",
      "   macro avg      0.600     0.646     0.621       352\n",
      "weighted avg      0.799     0.878     0.835       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.933     0.984     0.958       127\n",
      "           1      0.835     0.953     0.890       191\n",
      "           2      0.000     0.000     0.000        34\n",
      "\n",
      "    accuracy                          0.872       352\n",
      "   macro avg      0.589     0.646     0.616       352\n",
      "weighted avg      0.790     0.872     0.829       352\n",
      "\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:44.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.976     0.965       127\n",
      "           1      0.833     0.969     0.896       191\n",
      "           2      0.000     0.000     0.000        34\n",
      "\n",
      "    accuracy                          0.878       352\n",
      "   macro avg      0.596     0.648     0.620       352\n",
      "weighted avg      0.796     0.878     0.834       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.961     0.976     0.969       127\n",
      "           1      0.873     0.901     0.887       191\n",
      "           2      0.462     0.353     0.400        34\n",
      "\n",
      "    accuracy                          0.875       352\n",
      "   macro avg      0.765     0.743     0.752       352\n",
      "weighted avg      0.865     0.875     0.869       352\n",
      "\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:44.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.947     0.984     0.965       127\n",
      "           1      0.894     0.885     0.889       191\n",
      "           2      0.484     0.441     0.462        34\n",
      "\n",
      "    accuracy                          0.878       352\n",
      "   macro avg      0.775     0.770     0.772       352\n",
      "weighted avg      0.874     0.878     0.875       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.947     0.984     0.965       127\n",
      "           1      0.891     0.895     0.893       191\n",
      "           2      0.500     0.412     0.452        34\n",
      "\n",
      "    accuracy                          0.881       352\n",
      "   macro avg      0.779     0.764     0.770       352\n",
      "weighted avg      0.873     0.881     0.876       352\n",
      "\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:43.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.984     0.977       127\n",
      "           1      0.907     0.864     0.885       191\n",
      "           2      0.439     0.529     0.480        34\n",
      "\n",
      "    accuracy                          0.875       352\n",
      "   macro avg      0.772     0.793     0.780       352\n",
      "weighted avg      0.884     0.875     0.879       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.976     0.973       127\n",
      "           1      0.882     0.901     0.891       191\n",
      "           2      0.448     0.382     0.413        34\n",
      "\n",
      "    accuracy                          0.878       352\n",
      "   macro avg      0.766     0.753     0.759       352\n",
      "weighted avg      0.871     0.878     0.874       352\n",
      "\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.0025\n",
      "  Batch   160  of    176.    Elapsed: 0:00:44.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.976     0.973       127\n",
      "           1      0.891     0.895     0.893       191\n",
      "           2      0.469     0.441     0.455        34\n",
      "\n",
      "    accuracy                          0.881       352\n",
      "   macro avg      0.776     0.771     0.773       352\n",
      "weighted avg      0.878     0.881     0.879       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.976     0.973       127\n",
      "           1      0.891     0.895     0.893       191\n",
      "           2      0.469     0.441     0.455        34\n",
      "\n",
      "    accuracy                          0.881       352\n",
      "   macro avg      0.776     0.771     0.773       352\n",
      "weighted avg      0.878     0.881     0.879       352\n",
      "\n",
      "Found 7623 unique tokens.\n",
      "(1756, 256)\n",
      "Total 400000 word vectors.\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:43.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.969     0.969       127\n",
      "           1      0.831     0.979     0.899       191\n",
      "           2      0.000     0.000     0.000        34\n",
      "\n",
      "    accuracy                          0.881       352\n",
      "   macro avg      0.600     0.649     0.623       352\n",
      "weighted avg      0.800     0.881     0.837       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.976     0.973       127\n",
      "           1      0.835     0.979     0.901       191\n",
      "           2      0.000     0.000     0.000        34\n",
      "\n",
      "    accuracy                          0.884       352\n",
      "   macro avg      0.601     0.652     0.625       352\n",
      "weighted avg      0.803     0.884     0.840       352\n",
      "\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:44.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.983     0.937     0.960       127\n",
      "           1      0.853     0.911     0.881       191\n",
      "           2      0.407     0.324     0.361        34\n",
      "\n",
      "    accuracy                          0.864       352\n",
      "   macro avg      0.748     0.724     0.734       352\n",
      "weighted avg      0.857     0.864     0.859       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.984     0.969     0.976       127\n",
      "           1      0.873     0.901     0.887       191\n",
      "           2      0.400     0.353     0.375        34\n",
      "\n",
      "    accuracy                          0.872       352\n",
      "   macro avg      0.752     0.741     0.746       352\n",
      "weighted avg      0.867     0.872     0.870       352\n",
      "\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:44.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.984     0.969     0.976       127\n",
      "           1      0.880     0.880     0.880       191\n",
      "           2      0.389     0.412     0.400        34\n",
      "\n",
      "    accuracy                          0.866       352\n",
      "   macro avg      0.751     0.753     0.752       352\n",
      "weighted avg      0.870     0.866     0.868       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.976     0.973       127\n",
      "           1      0.901     0.859     0.879       191\n",
      "           2      0.429     0.529     0.474        34\n",
      "\n",
      "    accuracy                          0.869       352\n",
      "   macro avg      0.766     0.788     0.775       352\n",
      "weighted avg      0.880     0.869     0.874       352\n",
      "\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:44.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.961     0.969     0.965       127\n",
      "           1      0.887     0.901     0.894       191\n",
      "           2      0.500     0.441     0.469        34\n",
      "\n",
      "    accuracy                          0.881       352\n",
      "   macro avg      0.783     0.770     0.776       352\n",
      "weighted avg      0.876     0.881     0.878       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.961     0.969     0.965       127\n",
      "           1      0.874     0.911     0.892       191\n",
      "           2      0.480     0.353     0.407        34\n",
      "\n",
      "    accuracy                          0.878       352\n",
      "   macro avg      0.772     0.744     0.755       352\n",
      "weighted avg      0.868     0.878     0.872       352\n",
      "\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   160  of    176.    Elapsed: 0:00:45.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.969     0.969       127\n",
      "           1      0.892     0.869     0.881       191\n",
      "           2      0.436     0.500     0.466        34\n",
      "\n",
      "    accuracy                          0.869       352\n",
      "   macro avg      0.766     0.779     0.772       352\n",
      "weighted avg      0.876     0.869     0.872       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.969     0.969       127\n",
      "           1      0.897     0.869     0.883       191\n",
      "           2      0.450     0.529     0.486        34\n",
      "\n",
      "    accuracy                          0.872       352\n",
      "   macro avg      0.772     0.789     0.779       352\n",
      "weighted avg      0.880     0.872     0.876       352\n",
      "\n",
      "Found 7623 unique tokens.\n",
      "(1756, 256)\n",
      "Total 400000 word vectors.\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:45.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.961     0.976     0.969       127\n",
      "           1      0.845     0.974     0.905       191\n",
      "           2      1.000     0.088     0.162        34\n",
      "\n",
      "    accuracy                          0.889       352\n",
      "   macro avg      0.936     0.679     0.679       352\n",
      "weighted avg      0.902     0.889     0.856       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.961     0.976     0.969       127\n",
      "           1      0.834     0.974     0.899       191\n",
      "           2      0.000     0.000     0.000        34\n",
      "\n",
      "    accuracy                          0.881       352\n",
      "   macro avg      0.598     0.650     0.622       352\n",
      "weighted avg      0.799     0.881     0.837       352\n",
      "\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.01\n",
      "  Batch   160  of    176.    Elapsed: 0:00:45.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.961     0.969     0.965       127\n",
      "           1      0.890     0.806     0.846       191\n",
      "           2      0.353     0.529     0.424        34\n",
      "\n",
      "    accuracy                          0.838       352\n",
      "   macro avg      0.735     0.768     0.745       352\n",
      "weighted avg      0.864     0.838     0.848       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.932     0.976     0.954       127\n",
      "           1      0.841     0.942     0.889       191\n",
      "           2      0.200     0.029     0.051        34\n",
      "\n",
      "    accuracy                          0.866       352\n",
      "   macro avg      0.658     0.649     0.631       352\n",
      "weighted avg      0.812     0.866     0.831       352\n",
      "\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:44.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.976     0.976       127\n",
      "           1      0.906     0.806     0.853       191\n",
      "           2      0.364     0.588     0.449        34\n",
      "\n",
      "    accuracy                          0.847       352\n",
      "   macro avg      0.749     0.790     0.760       352\n",
      "weighted avg      0.879     0.847     0.859       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.976     0.976       127\n",
      "           1      0.901     0.853     0.876       191\n",
      "           2      0.409     0.529     0.462        34\n",
      "\n",
      "    accuracy                          0.866       352\n",
      "   macro avg      0.762     0.786     0.771       352\n",
      "weighted avg      0.880     0.866     0.872       352\n",
      "\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.005\n",
      "  Batch   160  of    176.    Elapsed: 0:00:44.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.961     0.976     0.969       127\n",
      "           1      0.911     0.911     0.911       191\n",
      "           2      0.562     0.529     0.545        34\n",
      "\n",
      "    accuracy                          0.898       352\n",
      "   macro avg      0.812     0.806     0.808       352\n",
      "weighted avg      0.895     0.898     0.897       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.961     0.976     0.969       127\n",
      "           1      0.911     0.906     0.908       191\n",
      "           2      0.545     0.529     0.537        34\n",
      "\n",
      "    accuracy                          0.895       352\n",
      "   macro avg      0.806     0.804     0.805       352\n",
      "weighted avg      0.894     0.895     0.894       352\n",
      "\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      " learning rate is:  0.0025\n",
      "  Batch   160  of    176.    Elapsed: 0:00:44.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.976     0.973       127\n",
      "           1      0.911     0.916     0.914       191\n",
      "           2      0.594     0.559     0.576        34\n",
      "\n",
      "    accuracy                          0.903       352\n",
      "   macro avg      0.825     0.817     0.821       352\n",
      "weighted avg      0.901     0.903     0.902       352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.976     0.973       127\n",
      "           1      0.911     0.916     0.914       191\n",
      "           2      0.594     0.559     0.576        34\n",
      "\n",
      "    accuracy                          0.903       352\n",
      "   macro avg      0.825     0.817     0.821       352\n",
      "weighted avg      0.901     0.903     0.902       352\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    import os\n",
    "    for seed in [42,52, 62, 72, 82]:\n",
    "        batch_size = 8\n",
    "        train_ids,test_ids,wd_ind,emb_ind=loadData_Tokenizer(X_train, X_test, MAX_NB_WORDS=10000,\n",
    "                       MAX_SEQUENCE_LENGTH=256, w2v_file=w2v_file)\n",
    "        embedding_matrix=init_embed(wd_ind, emb_ind, emb_dim=100)\n",
    "        ## initialize a classifier\n",
    "        clf_model=LSTMAttention(label_size=3, hidden_dim=60, batch_size=batch_size, lstm_layers=1, \n",
    "                keep_dropout=.5, embedding_matrix=torch.FloatTensor(embedding_matrix))\n",
    "        # Create the DataLoader for our training set.\n",
    "        train_data = TensorDataset(torch.LongTensor(train_ids), torch.tensor(y_train))\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "        # Create the DataLoader for our validation set.\n",
    "        validation_data = TensorDataset(torch.LongTensor(test_ids), torch.tensor(y_test))\n",
    "        validation_sampler = SequentialSampler(validation_data)\n",
    "        validation_dataloader = DataLoader(validation_data, sampler=validation_sampler,\n",
    "                                           batch_size=batch_size)\n",
    "        base_dir = 'textLSTM_att/LSTM_att_seed'+str(seed)\n",
    "        train_eval(clf_model, train_dataloader, validation_dataloader, base_dir, \\\n",
    "            lr=1.0e-2, epochs=5, eval_every_num_iters=160, seed_val = seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##text cnn\n",
    "res = np.array([[0.732,0.731,0.734,0.858],\n",
    "               [0.761,0.762,0.761,0.875],\n",
    "                [0.773,0.776,0.771,0.881],\n",
    "                [0.779,0.772,0.789,0.872],\n",
    "                [0.760,0.749,0.790,0.847]\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.761 , 0.758 , 0.769 , 0.8666])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(res, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01618641, 0.01640732, 0.02065914, 0.01237093])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(res, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##BERT\n",
    "res = np.array([[0.760,0.764,0.757,0.881],\n",
    "               [0.790,0.787,0.794,0.889],\n",
    "                [0.730,0.734,0.727,0.872],\n",
    "                [0.763,0.762,0.765,0.881]\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76075, 0.76175, 0.76075, 0.88075])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(res, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02125294, 0.01879328, 0.02385765, 0.0060156 ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(res, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
